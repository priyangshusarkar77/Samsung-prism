{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968a08d1",
   "metadata": {},
   "source": [
    "Description: In this notebook file, you can find the code regarding how to make a normal model into rotational invariant\n",
    "model. Here, we are using Resnet architecture style. We have also implemented color invariant block at the begining of the model.\n",
    "And in side rotational invariant block, only one layer is allowed for training and rest of the block of freezed. After training,\n",
    "that one layer, we will just transfer the kernel weight to the non-trainable layer with appropriate orientation. \n",
    "\n",
    "Issue: One issue we faced when working with the weights of the model is that is tampers the inference mechansim of the network.\n",
    "Using transferable weights, we can improve the feature detection ability of the model but it impacts the inference mechanism\n",
    "of the model. And this further reduces the accuracy of the model prediction. To handle this, after transfering the weights, we\n",
    "will set all the layers are trainable and train the model on the random augmented data for 2-3 epochs. And this adjusts the \n",
    "inference mechanism of the model and the model runs as intended.\n",
    "\n",
    "In our tests, the model which trained in this method shown similar accuracy irrespective of the rotation of the image. So, if \n",
    "we just improve the feature detection of the model, then we can simply transfer that knowledge to all orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d49433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2 as cv\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, LeakyReLU, DepthwiseConv2D, \\\n",
    "GlobalMaxPooling2D, Input, Dropout, Add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import pathlib as pl\n",
    "from itertools import permutations\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfc157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is related to Activation Suppression technique. This function takes input of normal image and applies\n",
    "# oil paint stylizing effect\n",
    "def oilpaint(img):\n",
    "    kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (2, 2))\n",
    "    morph = cv.morphologyEx(img, cv.MORPH_OPEN, kernel)\n",
    "    result = cv.normalize(morph, None, 50, 255, cv.NORM_MINMAX)\n",
    "    return result\n",
    "\n",
    "# This function is related to Activation Suppression, to get the coordinates of the activation regions\n",
    "def relative_coordinates_getter_circle(activation_area, dim):\n",
    "    pixel_ratio = 300/dim\n",
    "    relative_y = int((activation_area[1]/dim)*300)\n",
    "    relative_x = int((activation_area[0]/dim)*300)\n",
    "    return(relative_y, relative_x)\n",
    "\n",
    "# This function is related to Activation Suppression, it is used to blur any shape edges of the suppressions\n",
    "def diffuser(img, center, dim):\n",
    "    blured = cv.GaussianBlur(img, (31, 31), 0)\n",
    "    mask = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "    cv.circle(mask, (center), int(300/dim)+3, (255, 255, 255), -1)\n",
    "    \n",
    "    out = np.where(mask==np.array([255, 255, 255]), blured, img)\n",
    "    return out\n",
    " \n",
    "# This function is used to draw the circles on the regions where we want to suppress\n",
    "def supress_circle(img, center, dim):\n",
    "    dim += 4\n",
    "    img = oilpaint(img)\n",
    "    cv.circle(img, (center), int(300/dim), (127, 127, 127), -1)\n",
    "    return diffuser(img, center, dim)\n",
    "\n",
    "\n",
    "# This is the main function that handles everything related to activation suppression\n",
    "def activation_suppressor(model, taker_data, saver_data):\n",
    "    global class_names, counter\n",
    "    features = keras.Model(inputs=model.inputs, outputs=[layer.output for layer in model.layers])\n",
    "    layer_names = []\n",
    "    for layer in model.layers:\n",
    "        layer_names.append(layer.__class__.__name__)\n",
    "    not_req = [\"Flatten\", \"Dense\", \"Dropout\", \"GlobalMaxPooling2D\"]\n",
    "    req = []\n",
    "    for i in range(len(layer_names)):\n",
    "        if layer_names[i] not in not_req:\n",
    "            req.append(i)\n",
    "    for each_class in class_names:\n",
    "        temp_path = taker_data + \"\\\\\" + each_class\n",
    "        temp_pathlib_file = pl.Path(temp_path)\n",
    "        all_img = list(temp_pathlib_file.glob(\"*\"))\n",
    "        for each_img in all_img:\n",
    "            img = cv.imread(str(each_img))\n",
    "            img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "            img = cv.resize(img, (300, 300))\n",
    "            feature_extractor = features(np.expand_dims(img/255.0, 0))\n",
    "            l_features = feature_extractor[req[-1]].numpy()[0]\n",
    "            dim = len(l_features[..., 0])\n",
    "            final_img = np.zeros((dim, dim))\n",
    "            for i in range(len(l_features[0][0])):\n",
    "                final_img = final_img + l_features[..., i]\n",
    "            top_five_activations = []\n",
    "            for i in range(5):\n",
    "                activation_area_1 = np.where(final_img == np.max(final_img))\n",
    "                final_img[activation_area_1[0][0]][activation_area_1[1][0]] = 0\n",
    "                top_five_activations.append((activation_area_1[0][0],activation_area_1[1][0]) )\n",
    "            for i in range(5):\n",
    "                img = supress_circle(img, relative_coordinates_getter_circle(top_five_activations[i], dim-1), dim-1)\n",
    "            img = cv.cvtColor(img, cv.COLOR_RGB2BGR)\n",
    "            parts = str(each_img).split(\"\\\\\")[-1]\n",
    "            parts = parts.split(\".\")[-2]\n",
    "            cv.imwrite(saver_data + \"\\\\\" + each_class + \"\\\\aug\" + str(parts) + \"as.jpg\", img)\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9b27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining necessary paths of training and validation datasets\n",
    "training_path = \n",
    "validation_path = \n",
    "main_file_path = training_path\n",
    "main_dataset = pl.Path(main_file_path)\n",
    "class_names = np.array(sorted(item.name for item in main_dataset.glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a540f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These all functions are used to GPU implementation of the input pipeline of the tensorflow\n",
    "\n",
    "def get_label(file_name):\n",
    "    global class_names\n",
    "    parts = tf.strings.split(file_name, os.path.sep)\n",
    "    return parts[-2] == class_names\n",
    "\n",
    "def get_image(file_name):\n",
    "    file = tf.io.read_file(file_name)\n",
    "    image = tf.io.decode_image(contents=file, channels=3, expand_animations=False)\n",
    "    image = tf.image.resize(image, [300, 300])\n",
    "    image = image/255.0\n",
    "    return image\n",
    "\n",
    "def process_data(file_name):\n",
    "    label = get_label(file_name)\n",
    "    img = get_image(file_name)\n",
    "    return (img, label)\n",
    "\n",
    "def configure_input(dataset, batch_size):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size=batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6f3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This single function, handles everything related to the input pipeline of the training and validation datasets\n",
    "def input_pipeline(train_path, validation_path, train_batch_size, validation_batch_size):\n",
    "    global class_names\n",
    "    main_file_path = train_path\n",
    "    main_dataset = pl.Path(main_file_path)\n",
    "    #class_names = np.array(sorted(item.name for item in main_dataset.glob(\"*\")))\n",
    "    total_main_ds_size = len(list(main_dataset.glob(\"*/*\")))\n",
    "    \n",
    "    validation_file_path = validation_path\n",
    "    val_dataset = pl.Path(validation_file_path)\n",
    "    total_val_ds_size = len(list(val_dataset.glob(\"*/*\")))\n",
    "    \n",
    "    train_data_set = tf.data.Dataset.list_files(str(main_dataset/\"*/*\"), shuffle=False)\n",
    "    train_data_set = train_data_set.shuffle(total_main_ds_size, reshuffle_each_iteration=False)\n",
    "    \n",
    "    validation_data_set = tf.data.Dataset.list_files(str(val_dataset/\"*/*\"), shuffle=False)\n",
    "    validation_data_set = validation_data_set.shuffle(total_val_ds_size, reshuffle_each_iteration=False)\n",
    "    \n",
    "    print(f\"Size of the Training dataset: {tf.data.experimental.cardinality(train_data_set)}\")\n",
    "    print(f\"Size of the Testing dataset: {tf.data.experimental.cardinality(validation_data_set)}\")\n",
    "    \n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_data_set = train_data_set.map(process_data, num_parallel_calls=AUTOTUNE)\n",
    "    validation_data_set = validation_data_set.map(process_data, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    train_data_set = configure_input(train_data_set, train_batch_size)\n",
    "    validation_data_set = configure_input(validation_data_set, validation_batch_size)\n",
    "    \n",
    "    print(f\"Number of batches in training dataset: {tf.data.experimental.cardinality(train_data_set)}\")\n",
    "    print(f\"Number of batches in validation dataset: {tf.data.experimental.cardinality(validation_data_set)}\")\n",
    "    return train_data_set, validation_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31eae648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the Training dataset: 9000\n",
      "Size of the Testing dataset: 400\n",
      "Number of batches in training dataset: 1125\n",
      "Number of batches in validation dataset: 50\n"
     ]
    }
   ],
   "source": [
    "# Getting Training and Validation datasets\n",
    "train, val = input_pipeline(training_path, validation_path, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6898a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to returns block which contains a set of layers where each layer is associated with a particular order \n",
    "#of RGB channels. Here, only one layer is allowed for training and the rest of the layers are frozen and initialized with zeros.\n",
    "# The particular function is inspired from Inception architecture. Here, the input image is passed onto 6 layers (each layer)\n",
    "# represents a particular coombination of RGB channels. And in each layers, we perform convolution with 2 x 2 kernel (for better\n",
    "# texture detection) and 3 x 3 kernel (for better low-level feature detection). But unlike Inception architecture, we are not\n",
    "# concatenating the results, rather we are adding them. This is mainly to reduce the memory requirements of the model.\n",
    "\n",
    "\n",
    "def color_invariant_block(n_filters, l1_kernel_size, l2_kernel_size, X_input, block_code):\n",
    "    block_code=str(block_code)\n",
    "    texture1 = Conv2D(filters=n_filters, kernel_size=(l1_kernel_size, l1_kernel_size), padding=\"same\", name=\"trainable1\"+block_code)(X_input)\n",
    "    edges1 = Conv2D(filters=n_filters, kernel_size=(l2_kernel_size, l2_kernel_size), padding=\"same\", name=\"trainable2\"+block_code)(X_input)\n",
    "\n",
    "    texture2 = Conv2D(filters=n_filters, kernel_size=(l1_kernel_size, l1_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable1\"+block_code)(X_input)\n",
    "    edges2 = Conv2D(filters=n_filters, kernel_size=(l2_kernel_size, l2_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable2\"+block_code)(X_input)\n",
    "\n",
    "    texture3 = Conv2D(filters=n_filters, kernel_size=(l1_kernel_size, l1_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable3\"+block_code)(X_input)\n",
    "    edges3 = Conv2D(filters=n_filters, kernel_size=(l2_kernel_size, l2_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable4\"+block_code)(X_input)\n",
    "\n",
    "    texture4 = Conv2D(filters=n_filters, kernel_size=(l1_kernel_size, l1_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable5\"+block_code)(X_input)\n",
    "    edges4 = Conv2D(filters=n_filters, kernel_size=(l2_kernel_size, l2_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable6\"+block_code)(X_input)\n",
    "\n",
    "    texture5 = Conv2D(filters=n_filters, kernel_size=(l1_kernel_size, l1_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable7\"+block_code)(X_input)\n",
    "    edges5 = Conv2D(filters=n_filters, kernel_size=(l2_kernel_size, l2_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable8\"+block_code)(X_input)\n",
    "\n",
    "    texture6 = Conv2D(filters=n_filters, kernel_size=(l1_kernel_size, l1_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable9\"+block_code)(X_input)\n",
    "    edges6 = Conv2D(filters=n_filters, kernel_size=(l2_kernel_size, l2_kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable10\"+block_code)(X_input)\n",
    "\n",
    "    texture1 = LeakyReLU(alpha=0.0001)(texture1)\n",
    "    edges1 = LeakyReLU(alpha=0.001)(edges1)\n",
    "\n",
    "    texture2 = LeakyReLU(alpha=0.0001)(texture2)\n",
    "    edges2 = LeakyReLU(alpha=0.001)(edges2)\n",
    "\n",
    "    texture3 = LeakyReLU(alpha=0.0001)(texture3)\n",
    "    edges3 = LeakyReLU(alpha=0.001)(edges3)\n",
    "\n",
    "    texture4 = LeakyReLU(alpha=0.0001)(texture4)\n",
    "    edges4 = LeakyReLU(alpha=0.001)(edges4)\n",
    "\n",
    "    texture5 = LeakyReLU(alpha=0.0001)(texture5)\n",
    "    edges5 = LeakyReLU(alpha=0.001)(edges5)\n",
    "\n",
    "    texture6 = LeakyReLU(alpha=0.0001)(texture6)\n",
    "    edges6 = LeakyReLU(alpha=0.001)(edges6)\n",
    "\n",
    "    x1 = Add()([texture1, edges1])\n",
    "    x2 = Add()([texture2, edges2])\n",
    "    x3 = Add()([texture3, edges3])\n",
    "    x4 = Add()([texture4, edges4])\n",
    "    x5 = Add()([texture5, edges5])\n",
    "    x6 = Add()([texture6, edges6])\n",
    "    x = Add()([x1,x2,x3,x4,x5,x6])\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f154d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to return return a block which contains a set of layers which are associated with different angles.\n",
    "# In this function, we have 4 layers and only one layer is allowed to train. After training the one layer, the kernel weights\n",
    "# are rotated at 90, 180, 270 are transfered to the non-trainble layers. The main reason to choose only 90, 180, 270 is that\n",
    "# there are the angles which have highest variance between them, so model can capture more information and also, rotation of \n",
    "# kernel at arbitrary angles can introduce new weights in the kernel due to extrapolation.\n",
    "\n",
    "def rotational_equivariant_block(n_filters, kernel_size, input_layer, block_code):\n",
    "    block_code=str(block_code)\n",
    "    x1 = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), padding=\"same\", activation=\"relu\", name=\"trainable11\"+block_code)(input_layer)\n",
    "    x2 = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable11\"+block_code)(input_layer)\n",
    "    x3 = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable12\"+block_code)(input_layer)\n",
    "    x4 = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), padding=\"same\", activation=\"relu\", name=\"nontrainable13\"+block_code)(input_layer)\n",
    "    x = tf.keras.layers.maximum([x1, x2, x3, x4])\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f20aabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to mark which layers are allowed for training and which are now allowed for training\n",
    "def set_non_trainable(model):\n",
    "    for layer in model.layers:\n",
    "        name = layer.name\n",
    "        if name.startswith(\"nontrainable\"):\n",
    "            w = layer.get_weights()\n",
    "            new_w = np.zeros(w[0].shape)\n",
    "            new_b = np.zeros(layer.filters)\n",
    "            layer.set_weights([new_w, new_b])\n",
    "            layer.trainable_= False\n",
    "        elif name.startswith(\"nonzero\"):\n",
    "            layer.trainable = False\n",
    "        elif name.startswith(\"blur\"):\n",
    "            w = layer.get_weights()\n",
    "            new_w = np.zeros((w[0].shape))\n",
    "            new_w.fill(0.25)\n",
    "            layer.set_weights([new_w])\n",
    "            layer.trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "423557f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model with neccessary blocks and layers\n",
    "\n",
    "X_input = Input(shape=(300, 300, 3))\n",
    "n_filters=32\n",
    "X = color_invariant_block(n_filters=n_filters, l1_kernel_size=2, l2_kernel_size=3, X_input=X_input, block_code=1)\n",
    "X_copy = Conv2D(filters=n_filters, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(X_input)\n",
    "X = Add()([X, X_copy])\n",
    "X = DepthwiseConv2D(kernel_size=2, use_bias=False, padding=\"same\", name=\"blur1\")(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "X_copy = X\n",
    "n_filters=32\n",
    "X = rotational_equivariant_block(n_filters, 3, X, block_code=2)\n",
    "X_copy = Conv2D(filters=n_filters, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", name=\"nonzero1\")(X_copy)\n",
    "X = Add()([X, X_copy])\n",
    "X = DepthwiseConv2D(kernel_size=2, use_bias=False, padding=\"same\", name=\"blur2\")(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "\n",
    "X_copy = X\n",
    "n_filters=32\n",
    "X = rotational_equivariant_block(n_filters, 3 , X, block_code=3)\n",
    "X_copy = Conv2D(filters=n_filters, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", name=\"nonzero2\")(X_copy)\n",
    "X = Add()([X, X_copy])\n",
    "X = DepthwiseConv2D(kernel_size=2, use_bias=False, padding=\"same\", name=\"blur3\")(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "\n",
    "X_copy = X\n",
    "n_filters=32\n",
    "X = rotational_equivariant_block(n_filters, 3, X, block_code=4)\n",
    "X_copy = Conv2D(filters=n_filters, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", name=\"nonzero3\")(X_copy)\n",
    "X = Add()([X, X_copy])\n",
    "X = DepthwiseConv2D(kernel_size=2, use_bias=False, padding=\"same\", name=\"blur4\")(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "X_copy = X\n",
    "n_filters=32\n",
    "X = rotational_equivariant_block(n_filters, 3, X, block_code=5)\n",
    "X_copy = Conv2D(filters=n_filters, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", name=\"nonzero4\")(X_copy)\n",
    "X = Add()([X, X_copy])\n",
    "X = DepthwiseConv2D(kernel_size=2, use_bias=False, padding=\"same\", name=\"blur5\")(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "X = Dropout(0.2)(X)\n",
    "X = GlobalMaxPooling2D()(X)\n",
    "X = Dense(3, activation=\"softmax\")(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695af337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model with appropriate inputs and outputs\n",
    "model = Model(inputs=X_input, outputs=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534e0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the set_non_trainable function to mark which layers are allowed for training\n",
    "model = set_non_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74e6e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d73d0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 180s 153ms/step - loss: 0.8959 - accuracy: 0.6187 - val_loss: 2.5031 - val_accuracy: 0.1350\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "history = model.fit(train, validation_data=val, epochs=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b1390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling Activation Suppression function, to generate the suppressed data based on the models performance\n",
    "counter = 100001\n",
    "activation_suppressor(model, training_path, temporary_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b659807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this function, based on the block code, we will transfer weights from trainable layers to non-trainable layers with\n",
    "# necessary modifications\n",
    "\n",
    "def transfer_weights(model, block_code):\n",
    "    if block_code == 1: # For transfering color information\n",
    "        w1 = model.layers[1].get_weights()\n",
    "        w2 = model.layers[2].get_weights()\n",
    "        channels = [\"r\", \"g\", \"b\"]\n",
    "        combinations = permutations(channels)\n",
    "        c = 2\n",
    "        for i in combinations:\n",
    "            if c == 2:\n",
    "                c += 1\n",
    "            else:\n",
    "                new_w = np.zeros(w1[0].shape)\n",
    "                index = 0\n",
    "                for j in i:\n",
    "                    new_w[:, :, index, :] = w1[0][:, :, channels.index(j), :]\n",
    "                    index += 1\n",
    "                model.layers[c].set_weights([new_w, w1[1]])\n",
    "                c += 1\n",
    "                \n",
    "                new_w = np.zeros(w2[0].shape)\n",
    "                index = 0\n",
    "                for j in i:\n",
    "                    new_w[:, :, index, :] = w2[0][:, :, channels.index(j), :]\n",
    "                    index += 1\n",
    "                model.layers[c].set_weights([new_w, w2[1]])\n",
    "                c += 1\n",
    "    else: # For transfering rotational information\n",
    "        layer_index = 0\n",
    "        while not model.layers[layer_index].name.startswith(str(r\"nontrainable\")) or not model.layers[layer_index].name.endswith(str(block_code)):\n",
    "            layer_index += 1\n",
    "        layer_index -= 1\n",
    "        w = model.layers[layer_index].get_weights()\n",
    "        rotations = [90, 180, 270]\n",
    "        layer_index += 1\n",
    "        for i in rotations:\n",
    "            temp = w[0]\n",
    "            for j in range(w[0].shape[2]):\n",
    "                for k in range(w[0].shape[3]):\n",
    "                    temp[:, :, j, k] = ndimage.rotate(temp[:, :, j, k], i)\n",
    "            model.layers[layer_index].set_weights([temp, w[1]])\n",
    "            layer_index += 1\n",
    "\n",
    "    return model    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99046029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling transfer_weights function for all block codes\n",
    "for i in range(1, 6):\n",
    "    model = transfer_weights(model, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa346af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Defining an ImageDataGenerator to rotate an image at arbitrary angles so that we can retrain the transfered model so make sure\n",
    "# that the inference mechansim stays intact\n",
    "\n",
    "gen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=180\n",
    ")\n",
    "\n",
    "data = gen.flow_from_directory(r\"D:\\Samsung Prism\\Datasets\\Cat and Dog\\Training\",\n",
    "                              target_size=(300, 300), batch_size=8, class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b5d3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreezing all the layers and setting them to trainable\n",
    "for layer in model.layers:\n",
    "    if not layer.name.startswith(\"blur\"):\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e76a8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250/2250 [==============================] - 353s 156ms/step - loss: 0.9005 - accuracy: 0.5424 - val_loss: 0.7471 - val_accuracy: 0.5950\n"
     ]
    }
   ],
   "source": [
    "# Retraining the model for 5 or under 5 epochs\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "history = model.fit(data, validation_data=val, epochs=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60634ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 38). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model generated\\Temp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model generated\\Temp\\assets\n"
     ]
    }
   ],
   "source": [
    "# Finally, saving the model\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672bd593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
